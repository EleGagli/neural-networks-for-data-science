{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_6_Text_classification_with_neural_networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH-30c0dC8Ef",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks for Data Science Applications\n",
        "## Lab session 6: Text classification with neural networks\n",
        "\n",
        "**Contents of the lab session:**\n",
        "+ Using pre-trained word embeddings to classify text.\n",
        "+ Manually tokenize text and learn embeddings.\n",
        "+ Using TF Datasets and TF Hub for downloading datasets and modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAAix5fKPRXp",
        "colab_type": "code",
        "outputId": "f2e9e41a-d996-454b-f55d-666b42ecf3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "# Remember to enable a GPU on Colab by:\n",
        "# Runtime >> Change runtime type >> Hardware accelerator (before starting the VM).\n",
        "!pip -q install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 380.8MB 45kB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 45.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.1MB/s \n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_AE9PMgTJBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAzf-BsvDUXp",
        "colab_type": "text"
      },
      "source": [
        "### Download the dataset (IMDB movie reviews)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u3HW_H-TMOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you are running the code locally, you might need to install tensorflow_datasets first.\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6noIIYeTXcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print a list of all the available datasets\n",
        "print(tfds.list_builders())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKw-PCunT-8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Learn more about the dataset here: https://www.tensorflow.org/datasets/catalog/imdb_reviews\n",
        "# If you are on Windows and having errors while unzipping, you might need to increase the maximum allowed path length:\n",
        "# https://github.com/tensorflow/datasets/issues/769#issuecomment-515646783\n",
        "imdb = tfds.load('imdb_reviews', as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_BVDWF8VQLG",
        "colab_type": "code",
        "outputId": "bdf97220-c5df-4d87-c258-7d91ab964d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Inspect the object (a dictionary)\n",
        "imdb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': <_OptionsDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
              " 'train': <_OptionsDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
              " 'unsupervised': <_OptionsDataset shapes: ((), ()), types: (tf.string, tf.int64)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpdRBk3UVj89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only select the train part\n",
        "train_data = imdb['train']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhN5NOCHVnas",
        "colab_type": "code",
        "outputId": "c7c98e90-04e0-4cb0-9f4f-bd2b4f32343b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "# You can use it as you would use any tf.data.Dataset object\n",
        "for xb, yb in train_data.batch(4):\n",
        "    print(xb)\n",
        "    print(xb.shape)\n",
        "    print(yb)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"As a lifelong fan of Dickens, I have invariably been disappointed by adaptations of his novels.<br /><br />Although his works presented an extremely accurate re-telling of human life at every level in Victorian Britain, throughout them all was a pervasive thread of humour that could be both playful or sarcastic as the narrative dictated. In a way, he was a literary caricaturist and cartoonist. He could be serious and hilarious in the same sentence. He pricked pride, lampooned arrogance, celebrated modesty, and empathised with loneliness and poverty. It may be a clich\\xc3\\xa9, but he was a people's writer.<br /><br />And it is the comedy that is so often missing from his interpretations. At the time of writing, Oliver Twist is being dramatised in serial form on BBC television. All of the misery and cruelty is their, but non of the humour, irony, and savage lampoonery. The result is just a dark, dismal experience: the story penned by a journalist rather than a novelist. It's not really Dickens at all.<br /><br />'Oliver!', on the other hand, is much closer to the mark. The mockery of officialdom is perfectly interpreted, from the blustering beadle to the drunken magistrate. The classic stand-off between the beadle and Mr Brownlow, in which the law is described as 'a ass, a idiot' couldn't have been better done. Harry Secombe is an ideal choice.<br /><br />But the blinding cruelty is also there, the callous indifference of the state, the cold, hunger, poverty and loneliness are all presented just as surely as The Master would have wished.<br /><br />And then there is crime. Ron Moody is a treasure as the sleazy Jewish fence, whilst Oliver Reid has Bill Sykes to perfection.<br /><br />Perhaps not surprisingly, Lionel Bart - himself a Jew from London's east-end - takes a liberty with Fagin by re-interpreting him as a much more benign fellow than was Dicken's original. In the novel, he was utterly ruthless, sending some of his own boys to the gallows in order to protect himself (though he was also caught and hanged). Whereas in the movie, he is presented as something of a wayward father-figure, a sort of charitable thief rather than a corrupter of children, the latter being a long-standing anti-semitic sentiment. Otherwise, very few liberties are taken with Dickens's original. All of the most memorable elements are included. Just enough menace and violence is retained to ensure narrative fidelity whilst at the same time allowing for children' sensibilities. Nancy is still beaten to death, Bullseye narrowly escapes drowning, and Bill Sykes gets a faithfully graphic come-uppance.<br /><br />Every song is excellent, though they do incline towards schmaltz. Mark Lester mimes his wonderfully. Both his and my favourite scene is the one in which the world comes alive to 'who will buy'. It's schmaltzy, but it's Dickens through and through.<br /><br />I could go on. I could commend the wonderful set-pieces, the contrast of the rich and poor. There is top-quality acting from more British regulars than you could shake a stick at.<br /><br />I ought to give it 10 points, but I'm feeling more like Scrooge today. Soak it up with your Christmas dinner. No original has been better realised.\"\n",
            " b\"Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies i saw of her. And i have to say i feel in love with her, she was great in this move.<br /><br />Her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing.<br /><br />I hope this comment helps and u can buy the movie, the storyline is awesome is very unique and i'm sure u are going to like it. Jenna amazed us once more and no wonder the movie won so many awards. Her make-up and wardrobe is very very sexy and the girls on girls scene is amazing. specially the one where she looks like an angel. It's a must see and i hope u share my interests\"\n",
            " b\"I saw this film on True Movies (which automatically made me sceptical) but actually - it was good. Why? Not because of the amazing plot twists or breathtaking dialogue (of which there is little) but because actually, despite what people say I thought the film was accurate in it's depiction of teenagers dealing with pregnancy.<br /><br />It's NOT Dawson's Creek, they're not graceful, cool witty characters who breeze through sexuality with effortless knowledge. They're kids and they act like kids would. <br /><br />They're blunt, awkward and annoyingly confused about everything. Yes, this could be by accident and they could just be bad actors but I don't think so. Dermot Mulroney gives (when not trying to be cool) a very believable performance and I loved him for it. Patricia Arquette IS whiny and annoying, but she was pregnant and a teenagers? The combination of the two isn't exactly lavender on your pillow. The plot was VERY predictable and but so what? I believed them, his stress and inability to cope - her brave, yet slightly misguided attempts to bring them closer together. I think the characters, acted by anyone else, WOULD indeed have been annoying and unbelievable but they weren't. It reflects the surreality of the situation they're in, that he's sitting in class and she walks on campus with the baby. I felt angry at her for that, I felt angry at him for being such a child and for blaming her. I felt it all.<br /><br />In the end, I loved it and would recommend it.<br /><br />Watch out for the scene where Dermot Mulroney runs from the disastrous counselling session - career performance.\"\n",
            " b'This was a wonderfully clever and entertaining movie that I shall never tire of watching many, many times. The casting was magnificent in matching up the young with the older characters. There are those of us out here who really do appreciate good actors and an intelligent story format. As for Judi Dench, she is beautiful and a gift to any kind of production in which she stars. I always make a point to see Judi Dench in all her performances. She is a superb actress and a pleasure to watch as each transformation of her character comes to life. I can only be grateful when I see such an outstanding picture for most of the motion pictures made more recently lack good characters, good scripts and good acting. The movie public needs heroes, not deviant manikins, who lack ingenuity and talent. How wonderful to see old favorites like Leslie Caron, Olympia Dukakis and Cleo Laine. I would like to see this movie win the awards it deserves. Thank you again for a tremendous night of entertainment. I congratulate the writer, director, producer, and all those who did such a fine job.'], shape=(4,), dtype=string)\n",
            "(4,)\n",
            "tf.Tensor([1 1 1 1], shape=(4,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bsHj5WpExJO",
        "colab_type": "text"
      },
      "source": [
        "### First version: using a pre-trained text embedding module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK5311RLYwmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Like before, if you are running locally, you might need to install tensorflow_hub before.\n",
        "import tensorflow_hub as tfhub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5F3X0iPYz5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definitely read the documentation to understand what the module is doing!\n",
        "# TODO: test other text embedding modules.\n",
        "module_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwAfv_7IZa5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the module\n",
        "embedder = tfhub.load(module_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdNBMqxpZper",
        "colab_type": "code",
        "outputId": "42462a2e-5717-4e2b-8a5b-a6412ad862a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The module does tokenization + word embedding + reduction of all word embeddings to get a sentence embedding.\n",
        "embedder(xb).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlQvuvSDalcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can wrap the module inside a KerasLayer object to use it inside other models made up of Keras layers.\n",
        "embedder = tfhub.KerasLayer(module_url, dtype=tf.string, input_shape=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6OWSl7DbeqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential, layers, metrics, losses, optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEIgWVOLb6uE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: experiment with hidden layers before the final classification step.\n",
        "net = Sequential()\n",
        "net.add(embedder)\n",
        "net.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMq35AXQcYzv",
        "colab_type": "code",
        "outputId": "606e1e25-1b5d-4ab6-886f-302f695193fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Note: the text embeddings by default are non-trainable.\n",
        "net.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 128)               124642688 \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 124,642,817\n",
            "Trainable params: 129\n",
            "Non-trainable params: 124,642,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhOIopKidM50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = losses.BinaryCrossentropy()\n",
        "optimizer = optimizers.Adam()\n",
        "acc = metrics.BinaryAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tcjoZhWdoRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.compile(loss=loss, optimizer=optimizer, metrics=[acc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcbr7Xp-dwHE",
        "colab_type": "code",
        "outputId": "f087aa69-60e0-4d34-8c87-a3d270ed6259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "net.fit(train_data.shuffle(1000).batch(32), epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.6191 - binary_accuracy: 0.6876\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.5435 - binary_accuracy: 0.7544\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.5111 - binary_accuracy: 0.7703\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.4930 - binary_accuracy: 0.7763\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.4813 - binary_accuracy: 0.7798\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.4732 - binary_accuracy: 0.7830\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.4671 - binary_accuracy: 0.7849\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.4625 - binary_accuracy: 0.7867\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.4589 - binary_accuracy: 0.7881\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.4559 - binary_accuracy: 0.7889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5d8e255940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6wYqXL0eXf5",
        "colab_type": "code",
        "outputId": "99edf6bd-7f23-4c41-89d0-7c5bc7f7495b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Test on the test part\n",
        "net.evaluate(imdb['test'].batch(32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 12s 15ms/step - loss: 0.4611 - binary_accuracy: 0.7821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4611027718657423, 0.78212]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O_lcbcnlKk8",
        "colab_type": "code",
        "outputId": "8e0656fa-78b0-447c-f3aa-974cbd5bdc7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# TODO: try out with different sentences!\n",
        "xnew = tf.constant(['I hated this movie!'])\n",
        "net(xnew)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=27217, shape=(1, 1), dtype=float32, numpy=array([[0.3621955]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yY2Er7xFf6B",
        "colab_type": "text"
      },
      "source": [
        "### Second version: trainable embeddings with manual tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z54dfhwdlnRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import text, sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZZ75vKTmCmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the texts (not very elegant)\n",
        "train_texts = [t[0].numpy().decode('utf-8') for t in train_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il5zpB-0mlq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the labels\n",
        "train_labels = [t[1].numpy() for t in train_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSM_g7DXmxg5",
        "colab_type": "code",
        "outputId": "ab3d456b-1b46-4793-b0c7-ad2063d41ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_texts[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As a lifelong fan of Dickens, I have invariably been disappointed by adaptations of his novels.<br /><br />Although his works presented an extremely accurate re-telling of human life at every level in Victorian Britain, throughout them all was a pervasive thread of humour that could be both playful or sarcastic as the narrative dictated. In a way, he was a literary caricaturist and cartoonist. He could be serious and hilarious in the same sentence. He pricked pride, lampooned arrogance, celebrated modesty, and empathised with loneliness and poverty. It may be a cliché, but he was a people's writer.<br /><br />And it is the comedy that is so often missing from his interpretations. At the time of writing, Oliver Twist is being dramatised in serial form on BBC television. All of the misery and cruelty is their, but non of the humour, irony, and savage lampoonery. The result is just a dark, dismal experience: the story penned by a journalist rather than a novelist. It's not really Dickens at all.<br /><br />'Oliver!', on the other hand, is much closer to the mark. The mockery of officialdom is perfectly interpreted, from the blustering beadle to the drunken magistrate. The classic stand-off between the beadle and Mr Brownlow, in which the law is described as 'a ass, a idiot' couldn't have been better done. Harry Secombe is an ideal choice.<br /><br />But the blinding cruelty is also there, the callous indifference of the state, the cold, hunger, poverty and loneliness are all presented just as surely as The Master would have wished.<br /><br />And then there is crime. Ron Moody is a treasure as the sleazy Jewish fence, whilst Oliver Reid has Bill Sykes to perfection.<br /><br />Perhaps not surprisingly, Lionel Bart - himself a Jew from London's east-end - takes a liberty with Fagin by re-interpreting him as a much more benign fellow than was Dicken's original. In the novel, he was utterly ruthless, sending some of his own boys to the gallows in order to protect himself (though he was also caught and hanged). Whereas in the movie, he is presented as something of a wayward father-figure, a sort of charitable thief rather than a corrupter of children, the latter being a long-standing anti-semitic sentiment. Otherwise, very few liberties are taken with Dickens's original. All of the most memorable elements are included. Just enough menace and violence is retained to ensure narrative fidelity whilst at the same time allowing for children' sensibilities. Nancy is still beaten to death, Bullseye narrowly escapes drowning, and Bill Sykes gets a faithfully graphic come-uppance.<br /><br />Every song is excellent, though they do incline towards schmaltz. Mark Lester mimes his wonderfully. Both his and my favourite scene is the one in which the world comes alive to 'who will buy'. It's schmaltzy, but it's Dickens through and through.<br /><br />I could go on. I could commend the wonderful set-pieces, the contrast of the rich and poor. There is top-quality acting from more British regulars than you could shake a stick at.<br /><br />I ought to give it 10 points, but I'm feeling more like Scrooge today. Soak it up with your Christmas dinner. No original has been better realised.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHqjLpRjn9Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define and train a tokenizer on the 250 most popular words in our train corpus\n",
        "tokenizer = text.Tokenizer(num_words=250)\n",
        "tokenizer.fit_on_texts(train_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jzaBvaPpZPA",
        "colab_type": "code",
        "outputId": "5016fd3b-7486-4ac4-b140-48f8ff30d882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Dictionary {idx: word}, where indexes are ordered by frequency.\n",
        "tokenizer.index_word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'the',\n",
              " 2: 'and',\n",
              " 3: 'a',\n",
              " 4: 'of',\n",
              " 5: 'to',\n",
              " 6: 'is',\n",
              " 7: 'br',\n",
              " 8: 'in',\n",
              " 9: 'it',\n",
              " 10: 'i',\n",
              " 11: 'this',\n",
              " 12: 'that',\n",
              " 13: 'was',\n",
              " 14: 'as',\n",
              " 15: 'for',\n",
              " 16: 'with',\n",
              " 17: 'movie',\n",
              " 18: 'but',\n",
              " 19: 'film',\n",
              " 20: 'on',\n",
              " 21: 'not',\n",
              " 22: 'you',\n",
              " 23: 'are',\n",
              " 24: 'his',\n",
              " 25: 'have',\n",
              " 26: 'he',\n",
              " 27: 'be',\n",
              " 28: 'one',\n",
              " 29: 'all',\n",
              " 30: 'at',\n",
              " 31: 'by',\n",
              " 32: 'an',\n",
              " 33: 'they',\n",
              " 34: 'who',\n",
              " 35: 'so',\n",
              " 36: 'from',\n",
              " 37: 'like',\n",
              " 38: 'her',\n",
              " 39: 'or',\n",
              " 40: 'just',\n",
              " 41: 'about',\n",
              " 42: \"it's\",\n",
              " 43: 'out',\n",
              " 44: 'has',\n",
              " 45: 'if',\n",
              " 46: 'some',\n",
              " 47: 'there',\n",
              " 48: 'what',\n",
              " 49: 'good',\n",
              " 50: 'more',\n",
              " 51: 'when',\n",
              " 52: 'very',\n",
              " 53: 'up',\n",
              " 54: 'no',\n",
              " 55: 'time',\n",
              " 56: 'she',\n",
              " 57: 'even',\n",
              " 58: 'my',\n",
              " 59: 'would',\n",
              " 60: 'which',\n",
              " 61: 'only',\n",
              " 62: 'story',\n",
              " 63: 'really',\n",
              " 64: 'see',\n",
              " 65: 'their',\n",
              " 66: 'had',\n",
              " 67: 'can',\n",
              " 68: 'were',\n",
              " 69: 'me',\n",
              " 70: 'well',\n",
              " 71: 'than',\n",
              " 72: 'we',\n",
              " 73: 'much',\n",
              " 74: 'been',\n",
              " 75: 'bad',\n",
              " 76: 'get',\n",
              " 77: 'will',\n",
              " 78: 'do',\n",
              " 79: 'also',\n",
              " 80: 'into',\n",
              " 81: 'people',\n",
              " 82: 'other',\n",
              " 83: 'first',\n",
              " 84: 'great',\n",
              " 85: 'because',\n",
              " 86: 'how',\n",
              " 87: 'him',\n",
              " 88: 'most',\n",
              " 89: \"don't\",\n",
              " 90: 'made',\n",
              " 91: 'its',\n",
              " 92: 'then',\n",
              " 93: 'way',\n",
              " 94: 'make',\n",
              " 95: 'them',\n",
              " 96: 'too',\n",
              " 97: 'could',\n",
              " 98: 'any',\n",
              " 99: 'movies',\n",
              " 100: 'after',\n",
              " 101: 'think',\n",
              " 102: 'characters',\n",
              " 103: 'watch',\n",
              " 104: 'two',\n",
              " 105: 'films',\n",
              " 106: 'character',\n",
              " 107: 'seen',\n",
              " 108: 'many',\n",
              " 109: 'being',\n",
              " 110: 'life',\n",
              " 111: 'plot',\n",
              " 112: 'never',\n",
              " 113: 'acting',\n",
              " 114: 'little',\n",
              " 115: 'best',\n",
              " 116: 'love',\n",
              " 117: 'over',\n",
              " 118: 'where',\n",
              " 119: 'did',\n",
              " 120: 'show',\n",
              " 121: 'know',\n",
              " 122: 'off',\n",
              " 123: 'ever',\n",
              " 124: 'does',\n",
              " 125: 'better',\n",
              " 126: 'your',\n",
              " 127: 'end',\n",
              " 128: 'still',\n",
              " 129: 'man',\n",
              " 130: 'here',\n",
              " 131: 'these',\n",
              " 132: 'say',\n",
              " 133: 'scene',\n",
              " 134: 'while',\n",
              " 135: 'why',\n",
              " 136: 'scenes',\n",
              " 137: 'go',\n",
              " 138: 'such',\n",
              " 139: 'something',\n",
              " 140: 'through',\n",
              " 141: 'should',\n",
              " 142: 'back',\n",
              " 143: \"i'm\",\n",
              " 144: 'real',\n",
              " 145: 'those',\n",
              " 146: 'watching',\n",
              " 147: 'now',\n",
              " 148: 'though',\n",
              " 149: \"doesn't\",\n",
              " 150: 'years',\n",
              " 151: 'old',\n",
              " 152: 'thing',\n",
              " 153: 'actors',\n",
              " 154: 'work',\n",
              " 155: '10',\n",
              " 156: 'before',\n",
              " 157: 'another',\n",
              " 158: \"didn't\",\n",
              " 159: 'new',\n",
              " 160: 'funny',\n",
              " 161: 'nothing',\n",
              " 162: 'actually',\n",
              " 163: 'makes',\n",
              " 164: 'director',\n",
              " 165: 'look',\n",
              " 166: 'find',\n",
              " 167: 'going',\n",
              " 168: 'few',\n",
              " 169: 'same',\n",
              " 170: 'part',\n",
              " 171: 'again',\n",
              " 172: 'every',\n",
              " 173: 'lot',\n",
              " 174: 'cast',\n",
              " 175: 'us',\n",
              " 176: 'quite',\n",
              " 177: 'down',\n",
              " 178: 'want',\n",
              " 179: 'world',\n",
              " 180: 'things',\n",
              " 181: 'pretty',\n",
              " 182: 'young',\n",
              " 183: 'seems',\n",
              " 184: 'around',\n",
              " 185: 'got',\n",
              " 186: 'horror',\n",
              " 187: 'however',\n",
              " 188: \"can't\",\n",
              " 189: 'fact',\n",
              " 190: 'take',\n",
              " 191: 'big',\n",
              " 192: 'enough',\n",
              " 193: 'long',\n",
              " 194: 'thought',\n",
              " 195: \"that's\",\n",
              " 196: 'both',\n",
              " 197: 'between',\n",
              " 198: 'series',\n",
              " 199: 'give',\n",
              " 200: 'may',\n",
              " 201: 'original',\n",
              " 202: 'own',\n",
              " 203: 'action',\n",
              " 204: \"i've\",\n",
              " 205: 'right',\n",
              " 206: 'without',\n",
              " 207: 'always',\n",
              " 208: 'times',\n",
              " 209: 'comedy',\n",
              " 210: 'point',\n",
              " 211: 'gets',\n",
              " 212: 'must',\n",
              " 213: 'come',\n",
              " 214: 'role',\n",
              " 215: \"isn't\",\n",
              " 216: 'saw',\n",
              " 217: 'almost',\n",
              " 218: 'interesting',\n",
              " 219: 'least',\n",
              " 220: 'family',\n",
              " 221: 'done',\n",
              " 222: \"there's\",\n",
              " 223: 'whole',\n",
              " 224: 'bit',\n",
              " 225: 'music',\n",
              " 226: 'script',\n",
              " 227: 'far',\n",
              " 228: 'making',\n",
              " 229: 'guy',\n",
              " 230: 'anything',\n",
              " 231: 'feel',\n",
              " 232: 'minutes',\n",
              " 233: 'last',\n",
              " 234: 'since',\n",
              " 235: 'might',\n",
              " 236: 'performance',\n",
              " 237: \"he's\",\n",
              " 238: '2',\n",
              " 239: 'probably',\n",
              " 240: 'kind',\n",
              " 241: 'am',\n",
              " 242: 'away',\n",
              " 243: 'yet',\n",
              " 244: 'rather',\n",
              " 245: 'tv',\n",
              " 246: 'worst',\n",
              " 247: 'girl',\n",
              " 248: 'day',\n",
              " 249: 'sure',\n",
              " 250: 'fun',\n",
              " 251: 'hard',\n",
              " 252: 'woman',\n",
              " 253: 'played',\n",
              " 254: 'each',\n",
              " 255: 'found',\n",
              " 256: 'anyone',\n",
              " 257: 'having',\n",
              " 258: 'although',\n",
              " 259: 'especially',\n",
              " 260: 'our',\n",
              " 261: 'believe',\n",
              " 262: 'course',\n",
              " 263: 'comes',\n",
              " 264: 'looking',\n",
              " 265: 'screen',\n",
              " 266: 'trying',\n",
              " 267: 'set',\n",
              " 268: 'goes',\n",
              " 269: 'looks',\n",
              " 270: 'place',\n",
              " 271: 'book',\n",
              " 272: 'different',\n",
              " 273: 'put',\n",
              " 274: 'ending',\n",
              " 275: 'money',\n",
              " 276: 'maybe',\n",
              " 277: 'once',\n",
              " 278: 'sense',\n",
              " 279: 'reason',\n",
              " 280: 'true',\n",
              " 281: 'actor',\n",
              " 282: 'everything',\n",
              " 283: \"wasn't\",\n",
              " 284: 'shows',\n",
              " 285: 'dvd',\n",
              " 286: 'three',\n",
              " 287: 'worth',\n",
              " 288: 'year',\n",
              " 289: 'job',\n",
              " 290: 'main',\n",
              " 291: 'someone',\n",
              " 292: 'together',\n",
              " 293: 'watched',\n",
              " 294: 'play',\n",
              " 295: 'american',\n",
              " 296: 'plays',\n",
              " 297: '1',\n",
              " 298: 'said',\n",
              " 299: 'effects',\n",
              " 300: 'later',\n",
              " 301: 'takes',\n",
              " 302: 'instead',\n",
              " 303: 'seem',\n",
              " 304: 'beautiful',\n",
              " 305: 'john',\n",
              " 306: 'himself',\n",
              " 307: 'version',\n",
              " 308: 'audience',\n",
              " 309: 'high',\n",
              " 310: 'house',\n",
              " 311: 'night',\n",
              " 312: 'during',\n",
              " 313: 'everyone',\n",
              " 314: 'left',\n",
              " 315: 'special',\n",
              " 316: 'seeing',\n",
              " 317: 'half',\n",
              " 318: 'excellent',\n",
              " 319: 'wife',\n",
              " 320: 'star',\n",
              " 321: 'shot',\n",
              " 322: 'war',\n",
              " 323: 'idea',\n",
              " 324: 'nice',\n",
              " 325: 'black',\n",
              " 326: 'less',\n",
              " 327: 'mind',\n",
              " 328: 'simply',\n",
              " 329: 'read',\n",
              " 330: 'second',\n",
              " 331: 'else',\n",
              " 332: \"you're\",\n",
              " 333: 'father',\n",
              " 334: 'fan',\n",
              " 335: 'poor',\n",
              " 336: 'help',\n",
              " 337: 'completely',\n",
              " 338: 'death',\n",
              " 339: '3',\n",
              " 340: 'used',\n",
              " 341: 'home',\n",
              " 342: 'either',\n",
              " 343: 'short',\n",
              " 344: 'line',\n",
              " 345: 'given',\n",
              " 346: 'men',\n",
              " 347: 'top',\n",
              " 348: 'dead',\n",
              " 349: 'budget',\n",
              " 350: 'try',\n",
              " 351: 'performances',\n",
              " 352: 'wrong',\n",
              " 353: 'classic',\n",
              " 354: 'boring',\n",
              " 355: 'enjoy',\n",
              " 356: 'need',\n",
              " 357: 'rest',\n",
              " 358: 'use',\n",
              " 359: 'kids',\n",
              " 360: 'hollywood',\n",
              " 361: 'low',\n",
              " 362: 'production',\n",
              " 363: 'until',\n",
              " 364: 'along',\n",
              " 365: 'full',\n",
              " 366: 'friends',\n",
              " 367: 'camera',\n",
              " 368: 'truly',\n",
              " 369: 'women',\n",
              " 370: 'awful',\n",
              " 371: 'video',\n",
              " 372: 'next',\n",
              " 373: 'tell',\n",
              " 374: 'remember',\n",
              " 375: 'stupid',\n",
              " 376: 'couple',\n",
              " 377: 'start',\n",
              " 378: 'perhaps',\n",
              " 379: 'stars',\n",
              " 380: 'mean',\n",
              " 381: 'sex',\n",
              " 382: 'came',\n",
              " 383: 'recommend',\n",
              " 384: 'let',\n",
              " 385: 'moments',\n",
              " 386: 'wonderful',\n",
              " 387: 'episode',\n",
              " 388: 'understand',\n",
              " 389: 'small',\n",
              " 390: 'face',\n",
              " 391: 'terrible',\n",
              " 392: 'playing',\n",
              " 393: 'school',\n",
              " 394: 'getting',\n",
              " 395: 'written',\n",
              " 396: 'often',\n",
              " 397: 'doing',\n",
              " 398: 'keep',\n",
              " 399: 'early',\n",
              " 400: 'perfect',\n",
              " 401: 'name',\n",
              " 402: 'style',\n",
              " 403: 'human',\n",
              " 404: 'definitely',\n",
              " 405: 'gives',\n",
              " 406: 'others',\n",
              " 407: 'itself',\n",
              " 408: 'lines',\n",
              " 409: 'live',\n",
              " 410: 'become',\n",
              " 411: 'dialogue',\n",
              " 412: 'person',\n",
              " 413: 'lost',\n",
              " 414: 'finally',\n",
              " 415: 'piece',\n",
              " 416: 'head',\n",
              " 417: 'felt',\n",
              " 418: 'case',\n",
              " 419: 'yes',\n",
              " 420: 'liked',\n",
              " 421: 'supposed',\n",
              " 422: 'title',\n",
              " 423: \"couldn't\",\n",
              " 424: 'absolutely',\n",
              " 425: 'white',\n",
              " 426: 'against',\n",
              " 427: 'boy',\n",
              " 428: 'picture',\n",
              " 429: 'sort',\n",
              " 430: 'worse',\n",
              " 431: 'certainly',\n",
              " 432: 'went',\n",
              " 433: 'entire',\n",
              " 434: 'cinema',\n",
              " 435: 'waste',\n",
              " 436: 'problem',\n",
              " 437: 'hope',\n",
              " 438: 'entertaining',\n",
              " 439: \"she's\",\n",
              " 440: 'mr',\n",
              " 441: 'overall',\n",
              " 442: 'evil',\n",
              " 443: 'called',\n",
              " 444: 'loved',\n",
              " 445: 'based',\n",
              " 446: 'oh',\n",
              " 447: 'several',\n",
              " 448: 'fans',\n",
              " 449: 'mother',\n",
              " 450: 'drama',\n",
              " 451: 'beginning',\n",
              " 452: 'killer',\n",
              " 453: 'lives',\n",
              " 454: '5',\n",
              " 455: 'direction',\n",
              " 456: 'care',\n",
              " 457: 'already',\n",
              " 458: 'becomes',\n",
              " 459: 'laugh',\n",
              " 460: 'example',\n",
              " 461: 'friend',\n",
              " 462: 'dark',\n",
              " 463: 'despite',\n",
              " 464: 'under',\n",
              " 465: 'seemed',\n",
              " 466: 'throughout',\n",
              " 467: '4',\n",
              " 468: 'turn',\n",
              " 469: 'unfortunately',\n",
              " 470: 'wanted',\n",
              " 471: \"i'd\",\n",
              " 472: '\\x96',\n",
              " 473: 'children',\n",
              " 474: 'final',\n",
              " 475: 'fine',\n",
              " 476: 'history',\n",
              " 477: 'amazing',\n",
              " 478: 'sound',\n",
              " 479: 'guess',\n",
              " 480: 'heart',\n",
              " 481: 'totally',\n",
              " 482: 'humor',\n",
              " 483: 'lead',\n",
              " 484: 'writing',\n",
              " 485: 'michael',\n",
              " 486: 'quality',\n",
              " 487: \"you'll\",\n",
              " 488: 'close',\n",
              " 489: 'son',\n",
              " 490: 'wants',\n",
              " 491: 'guys',\n",
              " 492: 'works',\n",
              " 493: 'behind',\n",
              " 494: 'tries',\n",
              " 495: 'art',\n",
              " 496: 'side',\n",
              " 497: 'game',\n",
              " 498: 'past',\n",
              " 499: 'able',\n",
              " 500: 'b',\n",
              " 501: 'days',\n",
              " 502: 'turns',\n",
              " 503: \"they're\",\n",
              " 504: 'child',\n",
              " 505: 'hand',\n",
              " 506: 'enjoyed',\n",
              " 507: 'flick',\n",
              " 508: 'act',\n",
              " 509: 'genre',\n",
              " 510: 'town',\n",
              " 511: 'favorite',\n",
              " 512: 'soon',\n",
              " 513: 'kill',\n",
              " 514: 'starts',\n",
              " 515: 'sometimes',\n",
              " 516: 'gave',\n",
              " 517: 'car',\n",
              " 518: 'run',\n",
              " 519: 'late',\n",
              " 520: 'actress',\n",
              " 521: 'etc',\n",
              " 522: 'eyes',\n",
              " 523: 'directed',\n",
              " 524: 'horrible',\n",
              " 525: \"won't\",\n",
              " 526: 'brilliant',\n",
              " 527: 'viewer',\n",
              " 528: 'parts',\n",
              " 529: 'themselves',\n",
              " 530: 'self',\n",
              " 531: 'hour',\n",
              " 532: 'expect',\n",
              " 533: 'thinking',\n",
              " 534: 'stories',\n",
              " 535: 'stuff',\n",
              " 536: 'girls',\n",
              " 537: 'obviously',\n",
              " 538: 'blood',\n",
              " 539: 'decent',\n",
              " 540: 'city',\n",
              " 541: 'voice',\n",
              " 542: 'highly',\n",
              " 543: 'myself',\n",
              " 544: 'feeling',\n",
              " 545: 'fight',\n",
              " 546: 'except',\n",
              " 547: 'slow',\n",
              " 548: 'matter',\n",
              " 549: 'type',\n",
              " 550: 'kid',\n",
              " 551: 'anyway',\n",
              " 552: 'roles',\n",
              " 553: 'heard',\n",
              " 554: 'killed',\n",
              " 555: 'age',\n",
              " 556: 'says',\n",
              " 557: 'god',\n",
              " 558: 'moment',\n",
              " 559: 'took',\n",
              " 560: 'leave',\n",
              " 561: 'writer',\n",
              " 562: 'strong',\n",
              " 563: 'cannot',\n",
              " 564: 'violence',\n",
              " 565: 'police',\n",
              " 566: 'hit',\n",
              " 567: 'stop',\n",
              " 568: 'happens',\n",
              " 569: 'particularly',\n",
              " 570: 'known',\n",
              " 571: 'involved',\n",
              " 572: 'happened',\n",
              " 573: 'extremely',\n",
              " 574: 'obvious',\n",
              " 575: 'daughter',\n",
              " 576: 'chance',\n",
              " 577: 'told',\n",
              " 578: 'living',\n",
              " 579: 'coming',\n",
              " 580: 'lack',\n",
              " 581: 'experience',\n",
              " 582: 'alone',\n",
              " 583: \"wouldn't\",\n",
              " 584: 'including',\n",
              " 585: 'murder',\n",
              " 586: 'attempt',\n",
              " 587: 's',\n",
              " 588: 'please',\n",
              " 589: 'james',\n",
              " 590: 'happen',\n",
              " 591: 'wonder',\n",
              " 592: 'crap',\n",
              " 593: 'ago',\n",
              " 594: \"film's\",\n",
              " 595: 'brother',\n",
              " 596: 'gore',\n",
              " 597: 'complete',\n",
              " 598: 'none',\n",
              " 599: 'interest',\n",
              " 600: 'score',\n",
              " 601: 'group',\n",
              " 602: 'cut',\n",
              " 603: 'simple',\n",
              " 604: 'save',\n",
              " 605: 'looked',\n",
              " 606: 'ok',\n",
              " 607: 'hell',\n",
              " 608: 'career',\n",
              " 609: 'number',\n",
              " 610: 'song',\n",
              " 611: 'possible',\n",
              " 612: 'seriously',\n",
              " 613: 'annoying',\n",
              " 614: 'exactly',\n",
              " 615: 'sad',\n",
              " 616: 'shown',\n",
              " 617: 'running',\n",
              " 618: 'serious',\n",
              " 619: 'musical',\n",
              " 620: 'taken',\n",
              " 621: 'yourself',\n",
              " 622: 'whose',\n",
              " 623: 'released',\n",
              " 624: 'david',\n",
              " 625: 'cinematography',\n",
              " 626: 'scary',\n",
              " 627: 'ends',\n",
              " 628: 'hero',\n",
              " 629: 'usually',\n",
              " 630: 'english',\n",
              " 631: 'hours',\n",
              " 632: 'reality',\n",
              " 633: 'opening',\n",
              " 634: \"i'll\",\n",
              " 635: 'today',\n",
              " 636: 'light',\n",
              " 637: 'jokes',\n",
              " 638: 'across',\n",
              " 639: 'hilarious',\n",
              " 640: 'somewhat',\n",
              " 641: 'usual',\n",
              " 642: 'cool',\n",
              " 643: 'ridiculous',\n",
              " 644: 'body',\n",
              " 645: 'started',\n",
              " 646: 'level',\n",
              " 647: 'view',\n",
              " 648: 'relationship',\n",
              " 649: 'opinion',\n",
              " 650: 'change',\n",
              " 651: 'happy',\n",
              " 652: 'middle',\n",
              " 653: 'taking',\n",
              " 654: 'wish',\n",
              " 655: 'husband',\n",
              " 656: 'finds',\n",
              " 657: 'order',\n",
              " 658: 'saying',\n",
              " 659: 'talking',\n",
              " 660: 'shots',\n",
              " 661: 'documentary',\n",
              " 662: 'ones',\n",
              " 663: 'huge',\n",
              " 664: 'novel',\n",
              " 665: 'mostly',\n",
              " 666: 'female',\n",
              " 667: 'power',\n",
              " 668: 'robert',\n",
              " 669: 'episodes',\n",
              " 670: 'room',\n",
              " 671: 'important',\n",
              " 672: 'rating',\n",
              " 673: 'talent',\n",
              " 674: 'five',\n",
              " 675: 'major',\n",
              " 676: 'turned',\n",
              " 677: 'strange',\n",
              " 678: 'word',\n",
              " 679: 'modern',\n",
              " 680: 'call',\n",
              " 681: 'apparently',\n",
              " 682: 'disappointed',\n",
              " 683: 'single',\n",
              " 684: 'events',\n",
              " 685: 'due',\n",
              " 686: 'four',\n",
              " 687: 'songs',\n",
              " 688: 'attention',\n",
              " 689: 'basically',\n",
              " 690: '7',\n",
              " 691: 'knows',\n",
              " 692: 'clearly',\n",
              " 693: 'knew',\n",
              " 694: 'supporting',\n",
              " 695: 'television',\n",
              " 696: 'non',\n",
              " 697: 'british',\n",
              " 698: 'comic',\n",
              " 699: 'fast',\n",
              " 700: 'earth',\n",
              " 701: 'country',\n",
              " 702: 'class',\n",
              " 703: 'future',\n",
              " 704: 'cheap',\n",
              " 705: '8',\n",
              " 706: 'silly',\n",
              " 707: 'thriller',\n",
              " 708: 'king',\n",
              " 709: 'problems',\n",
              " 710: \"aren't\",\n",
              " 711: 'easily',\n",
              " 712: 'words',\n",
              " 713: 'tells',\n",
              " 714: 'jack',\n",
              " 715: 'miss',\n",
              " 716: 'local',\n",
              " 717: 'sequence',\n",
              " 718: 'bring',\n",
              " 719: 'entertainment',\n",
              " 720: 'paul',\n",
              " 721: 'beyond',\n",
              " 722: 'upon',\n",
              " 723: 'whether',\n",
              " 724: 'predictable',\n",
              " 725: 'moving',\n",
              " 726: 'romantic',\n",
              " 727: 'sets',\n",
              " 728: 'straight',\n",
              " 729: 'similar',\n",
              " 730: 'review',\n",
              " 731: 'falls',\n",
              " 732: 'oscar',\n",
              " 733: 'mystery',\n",
              " 734: 'enjoyable',\n",
              " 735: 'needs',\n",
              " 736: 'talk',\n",
              " 737: 'rock',\n",
              " 738: 'appears',\n",
              " 739: 'george',\n",
              " 740: 'giving',\n",
              " 741: 'eye',\n",
              " 742: 'within',\n",
              " 743: 'richard',\n",
              " 744: 'ten',\n",
              " 745: 'animation',\n",
              " 746: 'message',\n",
              " 747: 'theater',\n",
              " 748: 'near',\n",
              " 749: 'above',\n",
              " 750: 'dull',\n",
              " 751: 'sequel',\n",
              " 752: 'nearly',\n",
              " 753: 'points',\n",
              " 754: 'theme',\n",
              " 755: \"'\",\n",
              " 756: 'stand',\n",
              " 757: 'mention',\n",
              " 758: 'lady',\n",
              " 759: 'add',\n",
              " 760: 'bunch',\n",
              " 761: 'feels',\n",
              " 762: 'herself',\n",
              " 763: 'release',\n",
              " 764: 'red',\n",
              " 765: 'team',\n",
              " 766: 'storyline',\n",
              " 767: 'surprised',\n",
              " 768: 'ways',\n",
              " 769: 'named',\n",
              " 770: 'using',\n",
              " 771: \"haven't\",\n",
              " 772: 'easy',\n",
              " 773: 'lots',\n",
              " 774: 'fantastic',\n",
              " 775: 'begins',\n",
              " 776: 'actual',\n",
              " 777: 'working',\n",
              " 778: 'effort',\n",
              " 779: 'york',\n",
              " 780: 'french',\n",
              " 781: 'hate',\n",
              " 782: 'die',\n",
              " 783: 'minute',\n",
              " 784: 'tale',\n",
              " 785: 'clear',\n",
              " 786: 'stay',\n",
              " 787: '9',\n",
              " 788: 'elements',\n",
              " 789: 'feature',\n",
              " 790: 'among',\n",
              " 791: 'follow',\n",
              " 792: 're',\n",
              " 793: 'comments',\n",
              " 794: 'avoid',\n",
              " 795: 'viewers',\n",
              " 796: 'sister',\n",
              " 797: 'typical',\n",
              " 798: 'showing',\n",
              " 799: 'editing',\n",
              " 800: \"what's\",\n",
              " 801: 'tried',\n",
              " 802: 'famous',\n",
              " 803: 'sorry',\n",
              " 804: 'check',\n",
              " 805: 'fall',\n",
              " 806: 'dialog',\n",
              " 807: 'period',\n",
              " 808: 'form',\n",
              " 809: 'season',\n",
              " 810: 'certain',\n",
              " 811: 'filmed',\n",
              " 812: 'soundtrack',\n",
              " 813: 'weak',\n",
              " 814: 'means',\n",
              " 815: 'buy',\n",
              " 816: 'material',\n",
              " 817: 'realistic',\n",
              " 818: 'somehow',\n",
              " 819: 'crime',\n",
              " 820: 'figure',\n",
              " 821: 'doubt',\n",
              " 822: 'gone',\n",
              " 823: 'peter',\n",
              " 824: 'tom',\n",
              " 825: 'viewing',\n",
              " 826: 'kept',\n",
              " 827: 't',\n",
              " 828: 'general',\n",
              " 829: 'leads',\n",
              " 830: 'greatest',\n",
              " 831: 'space',\n",
              " 832: 'lame',\n",
              " 833: 'dance',\n",
              " 834: 'suspense',\n",
              " 835: 'imagine',\n",
              " 836: 'brought',\n",
              " 837: 'third',\n",
              " 838: 'atmosphere',\n",
              " 839: 'hear',\n",
              " 840: 'particular',\n",
              " 841: 'whatever',\n",
              " 842: 'sequences',\n",
              " 843: 'parents',\n",
              " 844: 'move',\n",
              " 845: 'lee',\n",
              " 846: 'indeed',\n",
              " 847: 'de',\n",
              " 848: 'eventually',\n",
              " 849: 'learn',\n",
              " 850: 'rent',\n",
              " 851: 'note',\n",
              " 852: 'forget',\n",
              " 853: 'reviews',\n",
              " 854: 'wait',\n",
              " 855: 'average',\n",
              " 856: 'deal',\n",
              " 857: 'japanese',\n",
              " 858: 'sexual',\n",
              " 859: 'poorly',\n",
              " 860: 'premise',\n",
              " 861: 'okay',\n",
              " 862: 'zombie',\n",
              " 863: 'surprise',\n",
              " 864: 'believable',\n",
              " 865: 'stage',\n",
              " 866: 'possibly',\n",
              " 867: 'sit',\n",
              " 868: \"who's\",\n",
              " 869: 'decided',\n",
              " 870: 'expected',\n",
              " 871: \"you've\",\n",
              " 872: 'subject',\n",
              " 873: 'nature',\n",
              " 874: 'became',\n",
              " 875: 'free',\n",
              " 876: 'difficult',\n",
              " 877: 'screenplay',\n",
              " 878: 'killing',\n",
              " 879: 'truth',\n",
              " 880: 'romance',\n",
              " 881: 'dr',\n",
              " 882: 'nor',\n",
              " 883: 'reading',\n",
              " 884: 'question',\n",
              " 885: 'needed',\n",
              " 886: 'leaves',\n",
              " 887: 'street',\n",
              " 888: '20',\n",
              " 889: 'meets',\n",
              " 890: 'hot',\n",
              " 891: 'begin',\n",
              " 892: 'unless',\n",
              " 893: 'baby',\n",
              " 894: 'otherwise',\n",
              " 895: 'superb',\n",
              " 896: 'imdb',\n",
              " 897: 'credits',\n",
              " 898: 'write',\n",
              " 899: 'shame',\n",
              " 900: \"let's\",\n",
              " 901: 'situation',\n",
              " 902: 'dramatic',\n",
              " 903: 'memorable',\n",
              " 904: 'directors',\n",
              " 905: 'earlier',\n",
              " 906: 'meet',\n",
              " 907: 'open',\n",
              " 908: 'dog',\n",
              " 909: 'disney',\n",
              " 910: 'badly',\n",
              " 911: 'weird',\n",
              " 912: 'male',\n",
              " 913: 'joe',\n",
              " 914: 'acted',\n",
              " 915: 'forced',\n",
              " 916: 'emotional',\n",
              " 917: 'laughs',\n",
              " 918: 'sci',\n",
              " 919: 'older',\n",
              " 920: 'realize',\n",
              " 921: 'society',\n",
              " 922: 'fi',\n",
              " 923: 'dream',\n",
              " 924: 'writers',\n",
              " 925: 'interested',\n",
              " 926: 'comment',\n",
              " 927: 'forward',\n",
              " 928: 'footage',\n",
              " 929: 'crazy',\n",
              " 930: 'deep',\n",
              " 931: 'whom',\n",
              " 932: 'america',\n",
              " 933: 'sounds',\n",
              " 934: 'beauty',\n",
              " 935: 'plus',\n",
              " 936: 'fantasy',\n",
              " 937: 'directing',\n",
              " 938: 'keeps',\n",
              " 939: 'ask',\n",
              " 940: 'development',\n",
              " 941: 'features',\n",
              " 942: 'air',\n",
              " 943: 'quickly',\n",
              " 944: 'mess',\n",
              " 945: 'creepy',\n",
              " 946: 'perfectly',\n",
              " 947: 'towards',\n",
              " 948: 'mark',\n",
              " 949: 'worked',\n",
              " 950: 'box',\n",
              " 951: 'unique',\n",
              " 952: 'cheesy',\n",
              " 953: 'setting',\n",
              " 954: 'hands',\n",
              " 955: 'plenty',\n",
              " 956: 'result',\n",
              " 957: 'previous',\n",
              " 958: 'brings',\n",
              " 959: 'total',\n",
              " 960: 'effect',\n",
              " 961: 'e',\n",
              " 962: 'incredibly',\n",
              " 963: 'personal',\n",
              " 964: 'fire',\n",
              " 965: 'rate',\n",
              " 966: 'monster',\n",
              " 967: 'business',\n",
              " 968: 'casting',\n",
              " 969: 'apart',\n",
              " 970: 'leading',\n",
              " 971: 'admit',\n",
              " 972: 'background',\n",
              " 973: 'appear',\n",
              " 974: 'powerful',\n",
              " 975: 'joke',\n",
              " 976: 'telling',\n",
              " 977: 'girlfriend',\n",
              " 978: 'meant',\n",
              " 979: 'christmas',\n",
              " 980: 'hardly',\n",
              " 981: 'present',\n",
              " 982: 'potential',\n",
              " 983: 'battle',\n",
              " 984: 'create',\n",
              " 985: 'bill',\n",
              " 986: 'break',\n",
              " 987: 'pay',\n",
              " 988: 'masterpiece',\n",
              " 989: 'gay',\n",
              " 990: 'political',\n",
              " 991: 'return',\n",
              " 992: 'dumb',\n",
              " 993: 'fighting',\n",
              " 994: 'fails',\n",
              " 995: 'various',\n",
              " 996: 'era',\n",
              " 997: 'portrayed',\n",
              " 998: 'secret',\n",
              " 999: 'cop',\n",
              " 1000: 'co',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq_bJ3v2p__e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word counts\n",
        "tokenizer.word_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSA0bG20qo1P",
        "colab_type": "code",
        "outputId": "82502bb3-f08f-4648-82cf-da80a23ef3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Tokenize the text: string --> sequence of integers corresponding to words inside the dictionary.\n",
        "xtoken = tokenizer.texts_to_sequences(train_texts[0:2])\n",
        "print(xtoken)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14, 3, 4, 10, 25, 74, 31, 4, 24, 7, 7, 24, 32, 4, 110, 30, 172, 8, 95, 29, 13, 3, 4, 12, 97, 27, 196, 39, 14, 1, 8, 3, 93, 26, 13, 3, 2, 26, 97, 27, 2, 8, 1, 169, 26, 2, 16, 2, 9, 200, 27, 3, 18, 26, 13, 3, 7, 7, 2, 9, 6, 1, 209, 12, 6, 35, 36, 24, 30, 1, 55, 4, 6, 109, 8, 20, 29, 4, 1, 2, 6, 65, 18, 4, 1, 2, 1, 6, 40, 3, 1, 62, 31, 3, 244, 71, 3, 42, 21, 63, 30, 29, 7, 7, 20, 1, 82, 6, 73, 5, 1, 1, 4, 6, 36, 1, 5, 1, 1, 122, 197, 1, 2, 8, 60, 1, 6, 14, 3, 25, 74, 125, 221, 6, 32, 7, 7, 18, 1, 6, 79, 47, 1, 4, 1, 1, 2, 23, 29, 40, 14, 14, 1, 59, 25, 7, 7, 2, 92, 47, 6, 6, 3, 14, 1, 44, 5, 7, 7, 21, 3, 36, 127, 3, 16, 31, 87, 14, 3, 73, 50, 71, 13, 201, 8, 1, 26, 13, 46, 4, 24, 202, 5, 1, 8, 5, 148, 26, 13, 79, 2, 8, 1, 17, 26, 6, 14, 139, 4, 3, 3, 4, 244, 71, 3, 4, 1, 109, 3, 193, 52, 168, 23, 16, 201, 29, 4, 1, 88, 23, 40, 192, 2, 6, 5, 30, 1, 169, 55, 15, 6, 128, 5, 2, 211, 3, 213, 7, 7, 172, 6, 148, 33, 78, 24, 196, 24, 2, 58, 133, 6, 1, 28, 8, 60, 1, 179, 5, 77, 42, 18, 42, 140, 2, 140, 7, 7, 10, 97, 137, 20, 10, 97, 1, 1, 4, 1, 2, 47, 6, 113, 36, 50, 71, 22, 97, 3, 30, 7, 7, 10, 5, 199, 9, 155, 18, 143, 50, 37, 9, 53, 16, 126, 54, 201, 44, 74, 125], [119, 9, 171, 11, 17, 9, 13, 28, 4, 1, 99, 10, 216, 4, 38, 2, 10, 25, 5, 132, 10, 231, 8, 116, 16, 38, 56, 13, 84, 8, 11, 7, 7, 38, 236, 13, 2, 48, 10, 1, 88, 13, 1, 2, 1, 9, 13, 22, 67, 12, 33, 3, 173, 80, 1, 17, 1, 68, 7, 7, 10, 11, 2, 67, 1, 17, 1, 6, 6, 52, 2, 143, 249, 23, 167, 5, 37, 9, 175, 50, 2, 54, 1, 17, 35, 108, 38, 94, 53, 2, 6, 52, 52, 2, 1, 20, 133, 6, 1, 28, 118, 56, 37, 32, 42, 3, 212, 64, 2, 10, 58]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt45J0w3rqX8",
        "colab_type": "code",
        "outputId": "276802ed-5310-4627-f821-50a25442dd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Sentences have varying lengths!\n",
        "print(len(xtoken[0]))\n",
        "print(len(xtoken[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "318\n",
            "111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI9f5VSWsHD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens = tokenizer.texts_to_sequences(train_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UBeRoFEsLp9",
        "colab_type": "code",
        "outputId": "ab4b914e-3a58-4d3b-e511-4fac7e7e62d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        }
      },
      "source": [
        "# Pad the sequences with zeros (optional: you can experiment adding a maximum length manually).\n",
        "sequence.pad_sequences(xtoken, padding='post')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 14,   3,   4,  10,  25,  74,  31,   4,  24,   7,   7,  24,  32,\n",
              "          4, 110,  30, 172,   8,  95,  29,  13,   3,   4,  12,  97,  27,\n",
              "        196,  39,  14,   1,   8,   3,  93,  26,  13,   3,   2,  26,  97,\n",
              "         27,   2,   8,   1, 169,  26,   2,  16,   2,   9, 200,  27,   3,\n",
              "         18,  26,  13,   3,   7,   7,   2,   9,   6,   1, 209,  12,   6,\n",
              "         35,  36,  24,  30,   1,  55,   4,   6, 109,   8,  20,  29,   4,\n",
              "          1,   2,   6,  65,  18,   4,   1,   2,   1,   6,  40,   3,   1,\n",
              "         62,  31,   3, 244,  71,   3,  42,  21,  63,  30,  29,   7,   7,\n",
              "         20,   1,  82,   6,  73,   5,   1,   1,   4,   6,  36,   1,   5,\n",
              "          1,   1, 122, 197,   1,   2,   8,  60,   1,   6,  14,   3,  25,\n",
              "         74, 125, 221,   6,  32,   7,   7,  18,   1,   6,  79,  47,   1,\n",
              "          4,   1,   1,   2,  23,  29,  40,  14,  14,   1,  59,  25,   7,\n",
              "          7,   2,  92,  47,   6,   6,   3,  14,   1,  44,   5,   7,   7,\n",
              "         21,   3,  36, 127,   3,  16,  31,  87,  14,   3,  73,  50,  71,\n",
              "         13, 201,   8,   1,  26,  13,  46,   4,  24, 202,   5,   1,   8,\n",
              "          5, 148,  26,  13,  79,   2,   8,   1,  17,  26,   6,  14, 139,\n",
              "          4,   3,   3,   4, 244,  71,   3,   4,   1, 109,   3, 193,  52,\n",
              "        168,  23,  16, 201,  29,   4,   1,  88,  23,  40, 192,   2,   6,\n",
              "          5,  30,   1, 169,  55,  15,   6, 128,   5,   2, 211,   3, 213,\n",
              "          7,   7, 172,   6, 148,  33,  78,  24, 196,  24,   2,  58, 133,\n",
              "          6,   1,  28,   8,  60,   1, 179,   5,  77,  42,  18,  42, 140,\n",
              "          2, 140,   7,   7,  10,  97, 137,  20,  10,  97,   1,   1,   4,\n",
              "          1,   2,  47,   6, 113,  36,  50,  71,  22,  97,   3,  30,   7,\n",
              "          7,  10,   5, 199,   9, 155,  18, 143,  50,  37,   9,  53,  16,\n",
              "        126,  54, 201,  44,  74, 125],\n",
              "       [119,   9, 171,  11,  17,   9,  13,  28,   4,   1,  99,  10, 216,\n",
              "          4,  38,   2,  10,  25,   5, 132,  10, 231,   8, 116,  16,  38,\n",
              "         56,  13,  84,   8,  11,   7,   7,  38, 236,  13,   2,  48,  10,\n",
              "          1,  88,  13,   1,   2,   1,   9,  13,  22,  67,  12,  33,   3,\n",
              "        173,  80,   1,  17,   1,  68,   7,   7,  10,  11,   2,  67,   1,\n",
              "         17,   1,   6,   6,  52,   2, 143, 249,  23, 167,   5,  37,   9,\n",
              "        175,  50,   2,  54,   1,  17,  35, 108,  38,  94,  53,   2,   6,\n",
              "         52,  52,   2,   1,  20, 133,   6,   1,  28, 118,  56,  37,  32,\n",
              "         42,   3, 212,  64,   2,  10,  58,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-bPOakBtNow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens = sequence.pad_sequences(train_tokens, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU04JWJAtT_m",
        "colab_type": "code",
        "outputId": "4097e965-b46f-44f9-e676-295c8cf36ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_tokens[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt0s6LMjtmLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re-insert inside a Dataset (again, not entirely elegant)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_tokens, train_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcHdQRhJt1rT",
        "colab_type": "code",
        "outputId": "b96558e7-0e46-49d7-804e-348a327f1a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for xb, yb in train_data.batch(4):\n",
        "  print(xb.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 1200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SADPM3wRvTYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define custom embedings (randomly initialized)\n",
        "embedder = layers.Embedding(250, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWWJQC68vnLd",
        "colab_type": "code",
        "outputId": "8a4ca920-9ab3-40aa-c0be-b1ff8d4e5305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedder(xb).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 1200, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQRwlqjNz3hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple model, like before\n",
        "netv2 = Sequential()\n",
        "netv2.add(embedder)\n",
        "netv2.add(layers.GlobalAvgPool1D())\n",
        "netv2.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UmvSj4z1DJk",
        "colab_type": "code",
        "outputId": "de92a387-f2c6-4904-a9b4-aa71237ebf77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "netv2(xb).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efvMyRYL1lN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "netv2.compile(\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    metrics=[acc]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATGIXPS32hjp",
        "colab_type": "code",
        "outputId": "7c2483e9-f927-4a1b-c5b5-49736c0ac3b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "netv2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 128)         32000     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 32,129\n",
            "Trainable params: 32,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLCHZ9WI2Fpo",
        "colab_type": "code",
        "outputId": "377311be-900d-40f0-b3de-a51a63b926f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "netv2.fit(train_data.shuffle(1000).batch(32),\n",
        "          epochs=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.6257 - binary_accuracy: 0.6593\n",
            "Epoch 2/3\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.5231 - binary_accuracy: 0.7611\n",
            "Epoch 3/3\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.4886 - binary_accuracy: 0.7762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5d408e6a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRYQjsIhGMnH",
        "colab_type": "text"
      },
      "source": [
        "### Version 3: convolutional neural network with dilated convolutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJzpphjO4Bzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a convolutional block\n",
        "def add_conv_block(model, filters, dilation):\n",
        "    # Conv1D\n",
        "    model.add(layers.Conv1D(filters, 5, \n",
        "                            dilation_rate=dilation))\n",
        "    # BatchNorm\n",
        "    model.add(layers.BatchNormalization())\n",
        "    # ReLU\n",
        "    model.add(layers.Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afQFvjcb1vgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: we are not considering masking here! This could be improved.\n",
        "# Also note how dilation is increasing further in the network. For larger networks,\n",
        "# we could have a repeating pattern 1/2/4/8/1/2/4/8 like in WaveNet.\n",
        "netv3 = Sequential()\n",
        "netv3.add(layers.Embedding(250, 128, mask_zero=True))\n",
        "add_conv_block(netv3, 64, 1)\n",
        "add_conv_block(netv3, 128, 2)\n",
        "add_conv_block(netv3, 256, 4)\n",
        "add_conv_block(netv3, 256, 8)\n",
        "netv3.add(layers.GlobalAvgPool1D())\n",
        "netv3.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya5DMxZ27414",
        "colab_type": "code",
        "outputId": "6eaaf41a-469d-4092-8cf7-0446a4e5f41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "source": [
        "netv3.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 128)         32000     \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 64)          41024     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 128)         41088     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 256)         164096    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, None, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, None, 256)         327936    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, None, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 609,217\n",
            "Trainable params: 607,809\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fpF9fte4A69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "netv3.compile(\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    metrics=[acc]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOW94hzP8Kqp",
        "colab_type": "code",
        "outputId": "4e030621-23a8-4580-8917-885b50618574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        }
      },
      "source": [
        "netv3.fit(train_data.shuffle(1000).batch(32),\n",
        "          epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 112s 143ms/step - loss: 0.4958 - binary_accuracy: 0.7624\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 107s 137ms/step - loss: 0.4473 - binary_accuracy: 0.7929\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 107s 137ms/step - loss: 0.4185 - binary_accuracy: 0.8118\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 107s 137ms/step - loss: 0.3947 - binary_accuracy: 0.8259\n",
            "Epoch 5/10\n",
            " 19/782 [..............................] - ETA: 1:46 - loss: 0.3362 - binary_accuracy: 0.8542"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-c57f989d8958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m netv3.fit(train_data.shuffle(1000).batch(32),\n\u001b[0;32m----> 2\u001b[0;31m           epochs=10)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmzJGGZ2-C9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Evaluate the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp7L6bTAGgAI",
        "colab_type": "text"
      },
      "source": [
        "### Save the embeddings for visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrViz_n1-Evt",
        "colab_type": "code",
        "outputId": "1e35af79-108d-4ae6-d969-16231857c799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Extract the embedding matrix:\n",
        "# i-th row: 128-dimensional embedding for the i-th word.\n",
        "for v in netv3.layers[0].trainable_variables:\n",
        "  print(v.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(250, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDFxZFR-5z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgxPsxlB_DTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need one TSV file for embeddings and one for words.\n",
        "# Read more here: http://projector.tensorflow.org/\n",
        "words_tsv = io.open('words.tsv', 'w', encoding='utf-8')\n",
        "embed_tsv = io.open('embeddings.tsv', 'w', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsMqknTJ__6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(250):\n",
        "    # Save the word in the file\n",
        "    word = tokenizer.index_word[idx + 1]\n",
        "    words_tsv.write(word + '\\n')\n",
        "\n",
        "    # Save the embedding vector (tab-separated)\n",
        "    word_embedding = v[idx].numpy()\n",
        "    tmp = '\\t'.join([ str(e) for e in word_embedding ])\n",
        "    embed_tsv.write(tmp + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ZFZk7JCJa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_tsv.close()\n",
        "embed_tsv.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccitHc3TGzSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If on Colab, download the two files (on the left menu, go to Files >> right click on the files >> Download)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApFsVlXIG7Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the files here and inspect them: http://projector.tensorflow.org/\n",
        "# TODO: looking at the embeddings, what do you see? What could be improved?"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}